{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering Presented with FoamTree V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "import webbrowser\n",
    "from sklearn.metrics import silhouette_score\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "stdin = sys.stdin\n",
    "stderr = sys.stderr\n",
    "reload(sys)\n",
    "sys.setdefaultencoding(\"utf-8\")\n",
    "sys.stdout = stdout\n",
    "sys.stdin = stdin\n",
    "sys.stderr = stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# path of text file, source dataset\n",
    "inputPath = 'C:/Users/liuxi/Desktop/TextClusteringPresentedWithFoamTree/publications.csv'\n",
    "\n",
    "# path where to save figs and txt file of clustering results\n",
    "savePath = 'C:/Users/liuxi/Desktop/TextClusteringPresentedWithFoamTree'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify parameters for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters (range 2 to 20) ----- 2\n"
     ]
    }
   ],
   "source": [
    "# number of initialization\n",
    "num_init = 100\n",
    "\n",
    "# number of interations\n",
    "num_iter = 300\n",
    "\n",
    "# tolerance with regards to convergence\n",
    "tolerance = 0.0001\n",
    "\n",
    "num_clusters = input('Number of clusters (range 2 to 20) ----- ') # specifying the number of clusters for K-Means Max = 20\n",
    "\n",
    "if num_clusters > 20 or num_clusters<2 or type(num_clusters)!=int:\n",
    "    print(' Warning !! Invalid INPUT !!! Please restart the program !!')\n",
    "    sys.exit()\n",
    "\n",
    "# specify the method of distance calculation.\n",
    "# options: cityblock, cosine, euclidean, l1, l2, manhattan\n",
    "distCal = 'euclidean'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Ghent library documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "catalog_file = inputPath\n",
    "catalog_entries = pd.read_csv(catalog_file, nrows=1000, usecols=['type', 'author', 'title', 'language'])\n",
    "english_catalog_entries = catalog_entries.loc[catalog_entries['language'] == 'eng'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge multi-feature into one column using pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many features are used for clustering (range 1 to 4) -2\n",
      " \n",
      "type    ---------------------------------------------1\n",
      "\n",
      "language---------------------------------------------2\n",
      "\n",
      "author  ---------------------------------------------3\n",
      "\n",
      "feature title has been chosen, choose the other feature - \n",
      "\n",
      "feature index number - 1\n"
     ]
    }
   ],
   "source": [
    "feature_dict = {0: english_catalog_entries.title,\n",
    "                1: english_catalog_entries.type,\n",
    "                2: english_catalog_entries.language,\n",
    "                3: english_catalog_entries.author}\n",
    "\n",
    "n = input('How many features are used for clustering (range 1 to 4) -')\n",
    "print(' ')\n",
    "print('type    ---------------------------------------------1\\n')\n",
    "print('language---------------------------------------------2\\n')\n",
    "print('author  ---------------------------------------------3\\n') \n",
    "\n",
    "if n==1:\n",
    "    df = pd.DataFrame({'0': feature_dict[0]})\n",
    "    df['multi-feature'] = df[['0']].apply(lambda x:' '.join(x), axis=1)               \n",
    "elif n==2:\n",
    "    print('feature title has been chosen, choose the other feature - \\n')\n",
    "    i = input('feature index number - ')\n",
    "    df = pd.DataFrame({'0': feature_dict[i], '1': feature_dict[0]})\n",
    "    df['multi-feature'] = df[['0', '1']].apply(lambda x:' '.join(x), axis=1)\n",
    "elif n==3:   \n",
    "    print('feature title has been chosen, choose the other two features - \\n')\n",
    "    i = input('feature index number - ')\n",
    "    j = input('feature index number - ')\n",
    "    df = pd.DataFrame({'0': feature_dict[i], '1': feature_dict[j], '2': feature_dict[0]})\n",
    "    df['multi-feature'] = df[['0', '1', '2']].apply(lambda x:' '.join(x), axis=1)\n",
    "elif n==4:\n",
    "    df = pd.DataFrame({'0': feature_dict[0], '1': feature_dict[1], '2': feature_dict[2], '3': feature_dict[3]})\n",
    "    df['multi-feature'] = df[['0', '1', '2','3']].apply(lambda x:' '.join(x), axis=1)\n",
    "else:\n",
    "    print(' Warning !! Invalid INPUT !!! Please restart the program !!')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define stopwords and stemmer for clearning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define stopwords and stemmer for clearning text\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "# To find unusual or mis-spelt words in a text corpus\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function of generating tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens if len(t) > 3 and t in english_vocab]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the parameters for TF-IDF calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features = 200000,\n",
    "                                   stop_words = 'english',\n",
    "                                   lowercase = True,\n",
    "                                   use_idf = True, \n",
    "                                   tokenizer = tokenize_and_stem,\n",
    "                                   ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tfidf_matrix returns (X, Y) Z, X is the index of docs, Y is the index of words in the dictionary, Z is the corresponding TF-IDF score of this word.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['multi-feature'] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms: A list of words in vocabulary. sorted by alphabetical order. given index can extract corresponding word in vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify which K-Means to use for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = num_clusters,n_init=num_init, max_iter=num_iter, tol=tolerance)\n",
    "km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2 The average silhouette_score is : 0.0140537055928\n"
     ]
    }
   ],
   "source": [
    "kmeans_model = KMeans(n_clusters=num_clusters,n_init=num_init, max_iter=num_iter,tol=tolerance).fit(tfidf_matrix)\n",
    "labels = kmeans_model.labels_\n",
    "x = silhouette_score(tfidf_matrix, labels)\n",
    "print(\"For n_clusters =\", num_clusters,\n",
    "          \"The average silhouette_score is :\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_catalog_entries.loc[:,'cluster_index'] = clusters \n",
    "e2 = english_catalog_entries.set_index('cluster_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km.cluster_centers_.argsort().shape\n",
    "km.cluster_centers_.argsort()[:, ::-1].shape\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize a list of saving the most relevant words for Foamtree representation\n",
    "myInput =[]\n",
    "counter = 0\n",
    "# generate a list of the most relevant words for Foamtree representation\n",
    "# for each cluster, save the top 10 most relevant words\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    for ind in order_centroids[i, :]:\n",
    "        if counter < 10*num_clusters:\n",
    "            if terms[ind] in english_vocab and len(terms[ind]) > 4:      \n",
    "                myInput.append(terms[ind])   \n",
    "                counter = counter+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate groups and labels for FoamTree representation which can be viewed in browser    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join(savePath, 'Text_Clustering.html')  \n",
    "with open(filename, 'w') as f:     \n",
    "    message2_2=''\n",
    "    for j in range(num_clusters):   \n",
    "        labelKey='label: \"%s %s %s %s %s\"'% (myInput[0],myInput[1],myInput[2], myInput[3], myInput[4])\n",
    "        del myInput[:10]\n",
    "        temp_article = '' \n",
    "        message21=''\n",
    "\n",
    "        for k in (e2.ix[j]['title']):  \n",
    "            article ='{label: \"%s\"},'% (k) \n",
    "            temp_article = temp_article + article\n",
    "        Collect_article = 'groups: [%s]'% (temp_article)\n",
    "        message2_1 = '{%s, %s},'%(labelKey, Collect_article)\n",
    "        message2_2 = message2_2 + message2_1\n",
    "        \n",
    "    message2 = 'groups: [' + '%s]'%(message2_2)\n",
    "\n",
    "    message1 = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>FoamTree Quick Start</title>\n",
    "    <meta charset=\"utf-8\" />\n",
    "  </head>\n",
    "\n",
    "  <body>\n",
    "    <div id=\"visualization\" style=\"width: 1800px; height: 1200px\"></div>\n",
    "    <script src=\"C:/Users/liuxi/Downloads/textClustering/JavaScript/carrotsearch.foamtree.js\"></script>\n",
    "    <script>\n",
    "      window.addEventListener(\"load\", function() {\n",
    "        var foamtree = new CarrotSearchFoamTree({\n",
    "          id: \"visualization\",\n",
    "          dataObject: {\"\"\"\n",
    "    message3 = \"\"\"\n",
    "          }\n",
    "        });\n",
    "      });\n",
    "    </script>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "\"\"\"\n",
    "    m = message1 + message2 + message3\n",
    "    f.write(m) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### options for 'metric': cityblock, cosine, euclidean, l1, l2, manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = 1 - pairwise_distances(tfidf_matrix, metric=distCal)\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=None)\n",
    "\n",
    "# Fit the data from dist, and returns the embedded coordinates.\n",
    "pos = mds.fit_transform(dist) \n",
    "\n",
    "# select x, y axis for making a plot\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "# sort data by x-values for a better view in figure\n",
    "xs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up colors dictionary\n",
    "cluster_colors = {0: 'r', 1: 'g', 2: 'k', 3: 'm', 4: 'b',5: 'r', 6: 'g', 7: 'k', 8: 'm', 9: 'b',\n",
    "                  10: 'r', 11: 'g', 12: 'k', 13: 'm', 14: 'b',15: 'r', 16: 'g', 17: 'k', 18: 'm', 19: 'b'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'Cluster 1',1: 'Cluster 2', 2: 'Cluster 3',3: 'Cluster 4',4: 'Cluster 5',5: 'Cluster 6', \n",
    "                 6: 'Cluster 7',7: 'Cluster 8', 8: 'Cluster 9',9: 'Cluster 10',10: 'Cluster 1',11: 'Cluster 2', \n",
    "                 12: 'Cluster 3',13: 'Cluster 4',14: 'Cluster 5',15: 'Cluster 6', \n",
    "                 16: 'Cluster 7',17: 'Cluster 8', 18: 'Cluster 9',19: 'Cluster 10'}               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=english_catalog_entries.cluster_index, title=english_catalog_entries.title)) \n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10)) \n",
    "ax.margins(0.05) \n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params( axis= 'x',\n",
    "                which='both',        \n",
    "                bottom='on',         \n",
    "                top='on',            \n",
    "                labelbottom='on')\n",
    "    ax.tick_params(axis= 'y',\n",
    "                which='both',       \n",
    "                left='on',          \n",
    "                top='on',           \n",
    "                labelleft='on')    \n",
    "    ax.legend(numpoints=1)  \n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "short_titles=[]\n",
    "for t in english_catalog_entries.title:\n",
    "    short_titles = short_titles+[t[:20]] \n",
    "    \n",
    "linkage_matrix = ward(dist) \n",
    "        \n",
    "fig, ax = plt.subplots(figsize=(10, 10)) \n",
    "ax = dendrogram(linkage_matrix, \n",
    "                    orientation=\"left\",\n",
    "                    labels=short_titles,\n",
    "                    show_leaf_counts=True,\n",
    "                    get_leaves=True,\n",
    "                    p=10,\n",
    "                    truncate_mode='lastp',\n",
    "                    distance_sort='descending',\n",
    "                    count_sort=True,\n",
    "                    show_contracted=True)\n",
    "plt.tick_params(axis= 'x',\n",
    "        which='both',      \n",
    "        bottom='on',      \n",
    "        top='on',        \n",
    "        labelbottom='on')\n",
    "#plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation finished !!!\n",
      "It took 28.4719998837 seconds.\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(savePath, 'Text_Clustering.html')  \n",
    "webbrowser.open_new_tab(filename)\n",
    "print('Calculation finished !!!')\n",
    "print ('It took', time.time()-start, 'seconds.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
